# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/178yBxZVMQJSzVv-CFdh1ja_yO03HhDT7

#Section 1: FastAPI Service & Endpoint
"""

# Install FastAPI and Uvicorn
# !pip install fastapi uvicorn

# Import FastAPI and required modules
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import uvicorn
from threading import Thread

# Create FastAPI app
app = FastAPI()

# Define input format with validation
class SymptomRequest(BaseModel):
    symptom_description: str = Field(..., min_length=5, max_length=500, description="Describe your symptoms in at least 5 characters.")

# Function to start FastAPI server in a background thread
def run():
    uvicorn.run(app, host="127.0.0.1", port=8000)

# Start the server
Thread(target=run, daemon=True).start()

"""#Section 2: LangChain Custom Tool & LLM Integration"""

# !pip install fastapi uvicorn langchain openai pydantic

from langchain.tools import tool

# Predefined list of specialists
SPECIALISTS = {
    "Cardiologist": "Heart and blood issues.",
    "Neurologist": "Brain, spinal cord, and nerves.",
    "Gastroenterologist": "Digestive system.",
    "Dermatologist": "Skin problems.",
    "Pulmonologist": "Lung and breathing issues.",
    "Orthopedist": "Bone and joint problems.",
    "General Practitioner": "General medical concerns."
}

# Custom tool to fetch specialist info
@tool
def specialist_info(specialist_name: str) -> str:
    """Returns a brief description of the specialist."""
    return SPECIALISTS.get(specialist_name, f"No information available for {specialist_name}.")

# Install Google Generative AI SDK
# !pip install -q google-generativeai

import google.generativeai as genai
import os

# Set up the API key
genai.configure(api_key="AIzaSyDFc7FK3pLCLoE01kC42BACaDwco8-DEO8")  # Replace with your actual API key

# Verify API key
print(os.getenv("GEMINI_API_KEY"))

os.environ["GEMINI_API_KEY"] = "AIzaSyDFc7FK3pLCLoE01kC42BACaDwco8-DEO8"  # Replace with your actual API key

print(os.getenv("GEMINI_API_KEY"))

# Load the model
model = genai.GenerativeModel("gemini-1.5-pro")

# Example test prompts
response = model.generate_content("Hello, how are you?")
print(response.text)

response = model.generate_content("Tell me a joke")
print(response.text)

"""#Section 3: Triage Agent Implementation & Prompting"""

from fastapi import HTTPException
import logging

# Configure logging for debugging AI steps
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def get_specialist(symptom: str):
    prompt = f"Analyze the symptoms and suggest the best specialist from this list: {', '.join(SPECIALISTS.keys())}. Symptom: {symptom}"

    try:
        logging.info(f"Agent received symptom: {symptom}")
        response = model.generate_content(prompt)
        response_text = response.text.strip()

        logging.info(f"Agent Response: {response_text}")

        # Default to General Practitioner if no match is found
        recommended_specialist = "General Practitioner"
        for specialist in SPECIALISTS.keys():
            if specialist.lower() in response_text.lower():
                recommended_specialist = specialist
                break

        logging.info(f"Recommended Specialist: {recommended_specialist}")

        return {
            "specialist": recommended_specialist,
            "explanation": response_text
        }

    except Exception as e:
        logging.error(f"Error in agent execution: {str(e)}")
        raise HTTPException(status_code=500, detail="Agent error")

"""#Section 4: API Endpoint for Symptom Analysis"""

# API Endpoint for Symptom Analysis
@app.post("/triage")
async def triage(request: SymptomRequest):
    try:
        result = get_specialist(request.symptom_description)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

"""#Section 5: Testing the API with Requests"""

# !pip install requests pandas

import requests
import pandas as pd

API_URL = "http://127.0.0.1:8000/triage"

data = {
    "symptom_description": "I have bone pain near my shoulder."
}

response = requests.post(API_URL, json=data)

# Parse the response JSON
response_json = response.json()

# Convert the response into a Pandas DataFrame
df = pd.DataFrame([response_json])

# Set pandas option to display longer text if necessary
pd.set_option('display.max_colwidth', None)

# Display the response as a table
print(df)

"""#Input Validation with Pydantic (Robust Request Validation)
Ensure symptoms contain only letters and spaces, convert to lowercase, and enforce length constraints.

"""

# Install dependencies

# from pydantic import BaseModel, Field, validator
from pydantic import BaseModel, Field, field_validator
import re

class SymptomRequest(BaseModel):
    symptom_description: str = Field(..., min_length=5, max_length=500, description="Describe your symptoms.")

    # @validator("symptom_description")
    @field_validator("symptom_description")
    def validate_symptom(cls, value):
        """Ensure symptoms contain only letters and spaces."""
        if not re.match(r"^[A-Za-z\s]+$", value):
            raise ValueError("Symptoms should only contain letters and spaces.")
        return value.strip().lower()  # Clean and format input

"""# 2. Agent Error Handling (Handling LLM & Tool Errors)
Catches AI errors and returns a 5xx response if the agent fails.
"""

import logging
from fastapi import HTTPException

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def get_specialist(symptom: str):
    prompt = f"Analyze the symptoms and suggest the best specialist from this list: {', '.join(SPECIALISTS.keys())}. Symptom: {symptom}"

    try:
        logging.info(f"Received symptom: {symptom}")

        response = model.generate_content(prompt)
        response_text = response.text.strip()

        if not response_text:
            raise ValueError("Empty response from AI model.")

        recommended_specialist = "General Practitioner"
        for specialist in SPECIALISTS.keys():
            if specialist.lower() in response_text.lower():
                recommended_specialist = specialist
                break

        logging.info(f"Recommended Specialist: {recommended_specialist}")

        return {
            "specialist": recommended_specialist,
            "explanation": response_text
        }

    except ValueError as ve:
        logging.error(f"Validation error: {str(ve)}")
        raise HTTPException(status_code=400, detail=f"Invalid input: {str(ve)}")

    except Exception as e:
        logging.error(f"Unexpected agent failure: {str(e)}")
        raise HTTPException(status_code=500, detail="Agent error: Unable to process request.")

"""#3. Agent Step Logging (Debugging Intermediate Steps)
Logs AI model responses and intermediate processing steps.
"""

import logging

# Save logs to a file
logging.basicConfig(filename="agent.log", level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def get_specialist(symptom: str):
    prompt = f"Analyze the symptoms and suggest the best specialist from this list: {', '.join(SPECIALISTS.keys())}. Symptom: {symptom}"

    try:
        logging.info(f"Received symptom: {symptom}")

        response = model.generate_content(prompt)
        response_text = response.text.strip()

        logging.info(f"Raw LLM Response: {response_text}")

        recommended_specialist = "General Practitioner"
        for specialist in SPECIALISTS.keys():
            if specialist.lower() in response_text.lower():
                recommended_specialist = specialist
                break

        logging.info(f"Final Recommended Specialist: {recommended_specialist}")

        return {
            "specialist": recommended_specialist,
            "explanation": response_text
        }

    except Exception as e:
        logging.error(f"Agent error: {str(e)}")
        raise HTTPException(status_code=500, detail="Agent encountered an error.")

"""#4. Basic Unit Testing (FastAPI & Mocking Agent)
Unit tests for valid, invalid inputs and agent failures.
"""

# Install pytest and httpx
# !pip install pytest httpx

# Save this file separately as test_main.py
from fastapi.testclient import TestClient
from unittest.mock import patch
import pytest
# from main import app  # Import FastAPI app

client = TestClient(app)  # Create test client

@pytest.fixture
def mock_specialist_response():
    """Mock AI agent response for testing."""
    return {
        "specialist": "Cardiologist",
        "explanation": "Based on the symptoms, a Cardiologist is recommended."
    }

def test_valid_symptom_request(mock_specialist_response):
    """Test valid symptom input."""
    with patch("main.get_specialist", return_value=mock_specialist_response):
        response = client.post("/triage", json={"symptom_description": "chest pain"})
        assert response.status_code == 200
        assert response.json() == mock_specialist_response

def test_invalid_symptom_request():
    """Test invalid symptom input (numbers/special characters)."""
    response = client.post("/triage", json={"symptom_description": "12345!"})
    assert response.status_code == 400
    assert "Invalid input" in response.json()["detail"]

def test_agent_failure():
    """Test when the agent fails."""
    with patch("main.get_specialist", side_effect=Exception("AI model error")):
        response = client.post("/triage", json={"symptom_description": "fever"})
        assert response.status_code == 500
        assert "Agent encountered an error" in response.json()["detail"]

"""#Section 6: Streamlit UI for Chatbot"""

# !pip install streamlit google-generativeai
# !npm install -g localtunnel

"""#Section 7: Deploying with LocalTunnel"""
import subprocess
import os

# Run curl command to access LocalTunnel
subprocess.run(["curl", "https://loca.lt/mytunnelpassword"], check=True)
print()



# Commented out IPython magic to ensure Python compatibility.
# %%writefile chatbot.py
# import streamlit as st
# import google.generativeai as genai

# # Ensure Gemini API is configured
# genai.configure(api_key="AIzaSyDFc7FK3pLCLoE01kC42BACaDwco8-DEO8")  # Replace with your actual API key

# # Load Gemini model
# model = genai.GenerativeModel("gemini-1.5-pro")

# # Streamlit UI
# st.title("🩺 Healthcare Triage Chatbot")
# st.write("Describe your symptoms, and I'll suggest the right specialist.")

# # Chat history
# if "messages" not in st.session_state:
#     st.session_state.messages = []

# # Display previous messages
# for message in st.session_state.messages:
#     with st.chat_message(message["role"]):
#         st.markdown(message["content"])

# # Input field
# user_input = st.chat_input("Describe your symptoms...")

# if user_input:
#     # Store user message
#     st.session_state.messages.append({"role": "user", "content": user_input})

#     with st.chat_message("user"):
#         st.markdown(user_input)

#     # Generate AI response
#     with st.spinner("Analyzing symptoms..."):
#         response = model.generate_content(f"Which medical specialist should I visit for these symptoms: {user_input}?")
#         bot_reply = response.text.strip()

#     with st.chat_message("assistant"):
#         st.markdown(bot_reply)

#     # Store AI response
#     st.session_state.messages.append({"role": "assistant", "content": bot_reply})


# """#Section 7: Deploying with LocalTunnel"""

# Run Streamlit app and LocalTunnel
subprocess.Popen(["streamlit", "run", "chatbot.py"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

print("Streamlit app is running, and LocalTunnel is exposing port 8501.")

# run 
# npx localtunnel --port 8501

# !curl https://loca.lt/mytunnelpassword

# !streamlit run chatbot.py & npx localtunnel --port 8501

